### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: meta-llama/Llama-3-8B-Instruct
#meta-llama/Llama-3.1-8B-Instruct
# ********************** #
adapter_name_or_path: ../saves/Meta-Llama-3-8B-Instruct/lora/dpo/msmarco_dpo/msmarco_dpo_2.0
template: llama3
finetuning_type: lora

### export
# ********************** #
export_dir: PLMs/trained_llm/Meta-Llama-3-8B-Instruct/dpo/lora/msmarco_dpo/msmarco_dpo_2.0
export_size: 2 
export_device: cpu
export_legacy_format: false